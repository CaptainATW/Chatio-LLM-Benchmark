\documentclass[11pt]{article}

%==============================================================================%
% Preamble
%==============================================================================%
\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{tcolorbox}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{array}
\usepackage{footmisc}

% Colors and link styles
\definecolor{chatioBlue}{HTML}{2563EB}
\definecolor{chatioGray}{HTML}{374151}
\definecolor{chatioLight}{HTML}{F3F4F6}
\definecolor{chatioAccent}{HTML}{059669}

\hypersetup{
  colorlinks=true,
  linkcolor=chatioBlue,
  citecolor=chatioBlue,
  urlcolor=chatioBlue
}

% Section formatting
\titleformat{\section}{\large\bfseries\color{chatioGray}}{\thesection.}{0.6em}{}
\titleformat{\subsection}{\normalsize\bfseries\color{chatioGray}}{\thesubsection}{0.6em}{}
\titleformat{\subsubsection}{\normalsize\bfseries\itshape\color{chatioGray}}{\thesubsubsection}{0.6em}{}

% List formatting
\setlist[itemize]{itemsep=0.25em, topsep=0.25em}
\setlist[enumerate]{itemsep=0.25em, topsep=0.25em}

% Boxes
\tcbset{
  colback=white,
  colframe=chatioBlue,
  arc=2mm,
  boxsep=1.5mm,
  left=2mm,right=2mm,top=2mm,bottom=2mm
}
\newtcolorbox{infobox}[1]{title={#1},colback=chatioLight,colframe=chatioBlue}
\newtcolorbox{notebox}[1]{title={#1},colframe=chatioAccent}

% Paragraph spacing
\setlength{\parskip}{6pt}
\setlength{\parindent}{0pt}

% Macros
\newcommand{\benchmark}{Chatio LLM Benchmark}
\newcommand{\product}{Chatio}
\newcommand{\version}{v1.0 (Release)}

%==============================================================================%
% Document
%==============================================================================%
\begin{document}

\begin{titlepage}
  \centering
  {\Large \textbf{\benchmark}\\[6pt]
  System Card}\\[1.0cm]
  {\normalsize \version \quad|\quad \today}\\[1.0cm]
  {\small Maintained by the Chatio Benchmark Team}\\[1.5cm]

  \begin{infobox}{At a Glance}
    \begin{itemize}
      \item \textbf{Structure:} Five distinct sub-benchmarks, each targeting a specific pillar of assistant utility.
      \item \textbf{Dataset:} ~500 closed-source prompts, synthetically generated with human verification. Created July 2025 to minimize contamination.
      \item \textbf{Evaluation:} A hybrid pipeline using:
        \begin{itemize}
            \item Human Expert Review (Volunteer Team) for empathy and helpfulness.
            \item Ensemble LLM-as-a-Judge (Claude 4.5 Sonnet + GPT-5.1) for creativity.
            \item Deterministic/Hardcoded checks for instruction following.
        \end{itemize}
      \item \textbf{Scope:} English-only, single-turn/short-context interactions.
      \item \textbf{Intended Use:} Evaluating real-world helpfulness, tone adaptation, and everyday logic; not for code generation or math olympiad capability.
    \end{itemize}
  \end{infobox}

  \vfill
  {\small This system card documents the intent, design, evaluation methods, ethics, limitations, and update policy for the \product{} benchmark.}
\end{titlepage}

\tableofcontents
\newpage

\section{Purpose and Scope}

The \benchmark{} evaluates language models on practical, real-world tasks that reflect everyday user needs. Unlike academic benchmarks that prioritize rote knowledge or puzzle-solving, \product{} measures "assistant fit", or how well a model integrates into daily life. The benchmark is divided into five distinct sub-benchmarks, each providing a standalone score that contributes to the final rating.

\section{Task Design: The Five Pillars}

\product{} consists of five separate testing tracks:

\subsection{1. Emotional Support (Human-Evaluated)}
Models are presented with scenarios involving user distress, anxiety, or conflict.
\begin{itemize}
    \item \textbf{Goal:} Provide comfort and validation without sounding robotic.
    \item \textbf{Key Metric:} Tone adaptation. Does the model shift its voice to match the severity of the situation?
\end{itemize}

\subsection{2. Creative Writing (Ensemble AI-Evaluated)}
Models generate original content such as short stories, emails, or dialogues based on open-ended prompts.
\begin{itemize}
    \item \textbf{Goal:} produce engaging, coherent, and original text.
    \item \textbf{Key Metric:} Narrative flow and stylistic adherence.
\end{itemize}

\subsection{3. Instruction Following (Hybrid Hardcoded)}
Tasks require strict adherence to constraints (e.g., "no adjectives," "JSON format only," "under 50 words").
\begin{itemize}
    \item \textbf{Goal:} Precision and reliability.
    \item \textbf{Key Metric:} Constraint satisfaction rate (Pass/Fail).
\end{itemize}

\subsection{4. Reading Comprehension (Hardcoded)}
Models are given text passages and must answer specific questions or extract data.
\begin{itemize}
    \item \textbf{Goal:} Zero-hallucination information retrieval.
    \item \textbf{Key Metric:} Factual accuracy and inclusion of required details.
\end{itemize}

\subsection{5. General Helpfulness (Dual-Faceted)}
This category covers general knowledge explanation and real-world logistics (e.g., "How do I fix a leaky faucet?").
\begin{itemize}
    \item \textbf{Goal:} Actionable, clear advice.
    \item \textbf{Key Metric:} Clarity of explanation and correctness of advice.
\end{itemize}

\section{Evaluation Methodology}

\product{} employs a specialized evaluation pipeline for each of the five benchmarks to ensure the scoring method matches the nature of the task.

\subsection{Empathy \& General Helpfulness: Human Pairwise Comparison}
Subjective categories are evaluated by a dedicated team of 3--5 human volunteers (comprising the authors and CS students).
\begin{itemize}
    \item \textbf{Method:} Blind side-by-side comparison. Reviewers see the output of Model A and Model B without identifying labels.
    \item \textbf{Criteria:} Reviewers vote on which response is more comforting (for Empathy) or more actionable (for Helpfulness).
    \item \textbf{Nuance:} Humans specifically check for "tone adaptation"—rewarding models that sound authentic rather than using canned "I understand" templates.
\end{itemize}

\subsection{Creative Writing: Ensemble LLM-as-a-Judge}
Evaluating creativity is notoriously difficult for a single model due to inherent biases (e.g., one model preferring flowery language, another preferring conciseness).
\begin{itemize}
    \item \textbf{Judges:} We utilize an ensemble of \textbf{Claude 4.5 Sonnet} and \textbf{GPT-5.1}.
    \item \textbf{Rationale:} By averaging the scores of two distinct state-of-the-art models with different training lineages, we mitigate individual judge bias and achieve a more neutral, high-fidelity score.
\end{itemize}

\subsection{Instruction Following \& Comprehension: Programmatic Validation}
These tasks are objective and are scored using hardcoded test suites.
\begin{itemize}
    \item \textbf{Method:} Python-based string parsing, Regex, and JSON validation.
    \item \textbf{Human Fallback:} In cases where programmatic parsing fails (e.g., ambiguous formatting), the sample is flagged for human review to ensure the model is not penalized for a valid but unexpected output format.
\end{itemize}

\subsection{General Helpfulness: Two-Faceted Approach}
This benchmark combines objective and subjective measures:
\begin{enumerate}
    \item \textbf{Factual Accuracy:} A question bank of real-world queries is checked for factual correctness.
    \item \textbf{Human Preference:} The human volunteer team ranks responses based on "helpfulness"—how easy the advice is to follow for a layperson.
\end{enumerate}

\section{Test Configuration and Decontamination}

To ensure fair and reproducible results, we adhere to strict configuration and data hygiene standards.

\subsection{Prompt Dataset}
\begin{itemize}
    \item \textbf{Volume:} Approximately 500 unique prompts across the five categories.
    \item \textbf{Origin:} Prompts are synthetically generated to ensure diversity, then manually reviewed and filtered by the human team for quality.
    \item \textbf{Decontamination:} The dataset was created in \textbf{July 2025}. This date post-dates the training data cutoff for many current models, reducing the risk of memorization. Furthermore, the benchmark is closed-source, and evaluations are conducted via providers with zero-data-retention policies to prevent future contamination.
\end{itemize}

\subsection{Inference Settings}
\begin{itemize}
    \item \textbf{Temperature:} We strictly use the provider's recommended temperature settings for each model (typically range 0.7--1.0).
    \item \textbf{Default:} If no specific recommendation exists, we default to a temperature of \textbf{1.0} to allow for natural variance in creative tasks.
    \item \textbf{Context:} The benchmark focuses on short-context interactions typical of daily assistant usage.
\end{itemize}

\section{Ethical Considerations}

\subsection{Bias and Fairness}
LLMs can inadvertently produce biased or stereotypical content. \product{} includes tasks that surface differential treatment across genders, backgrounds, or occupations. Human evaluators apply fairness guidelines and watch for subtle biases.

\subsection{Safety and Harmful Content}
The benchmark avoids tasks encouraging disallowed content. Evaluators consider safety risks; blatantly harmful or abusive responses are noted and penalized. Helpfulness must align with ethics: superior performance requires both capability and responsible behavior.

\section{Limitations}

\begin{itemize}
    \item \textbf{Dataset Size:} With ~500 prompts, this benchmark is smaller than massive academic datasets. It is designed as a high-quality "probe" rather than an exhaustive capabilities test.
    \item \textbf{Scope:} Focused on English text only. No multimodal, coding, or advanced math capabilities are tested.
    \item \textbf{Human Subjectivity:} While our volunteer team follows rubrics, human evaluation inherently contains some degree of subjectivity regarding what constitutes "helpful" or "empathetic."
\end{itemize}


\newpage
\begin{thebibliography}{99}

\bibitem{prompteng2024}
PromptEngineering.org. “Challenges and Innovations in Language Model Benchmarking and Generalization.” 2024.

\bibitem{livebench2025}
Emergent Mind. “LiveBench: Dynamic LLM Benchmark Suite.” 2025.

\bibitem{galileo-judge-2024}
Galileo AI Blog. “LLM-as-a-Judge vs Human Evaluation.” 2024.

\bibitem{geval-2023}
Confident AI Blog. “G-Eval Simply Explained: LLM-as-a-Judge for LLM Evaluation.” 2023.

\bibitem{citadel-2024}
Citadel AI Blog. “Announcing Lens for LLMs: Combining Human and Automated LLM Evaluation.” 2024.

\bibitem{phare-2025}
Giskard (Phare Benchmark). “LLMs recognise bias but also reproduce harmful stereotypes.” 2025.

\bibitem{blend-2024}
Myung et al. “BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages.” arXiv, 2024.

\bibitem{e-bench2025}
Galileo AI Blog. “Evaluating LLM Ease-of-Use Through the E-Bench Framework.” 2025.

\end{thebibliography}

\end{document}