\documentclass[11pt]{article}

%==============================================================================%
% Preamble
%==============================================================================%
\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{tcolorbox}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{array}
\usepackage{footmisc}

% Colors and link styles
\definecolor{chatioBlue}{HTML}{2563EB}
\definecolor{chatioGray}{HTML}{374151}
\definecolor{chatioLight}{HTML}{F3F4F6}
\definecolor{chatioAccent}{HTML}{059669}

\hypersetup{
  colorlinks=true,
  linkcolor=chatioBlue,
  citecolor=chatioBlue,
  urlcolor=chatioBlue
}

% Section formatting
\titleformat{\section}{\large\bfseries\color{chatioGray}}{\thesection.}{0.6em}{}
\titleformat{\subsection}{\normalsize\bfseries\color{chatioGray}}{\thesubsection}{0.6em}{}
\titleformat{\subsubsection}{\normalsize\bfseries\itshape\color{chatioGray}}{\thesubsubsection}{0.6em}{}

% List formatting
\setlist[itemize]{itemsep=0.25em, topsep=0.25em}
\setlist[enumerate]{itemsep=0.25em, topsep=0.25em}

% Boxes
\tcbset{
  colback=white,
  colframe=chatioBlue,
  arc=2mm,
  boxsep=1.5mm,
  left=2mm,right=2mm,top=2mm,bottom=2mm
}
\newtcolorbox{infobox}[1]{title={#1},colback=chatioLight,colframe=chatioBlue}
\newtcolorbox{notebox}[1]{title={#1},colframe=chatioAccent}

% Paragraph spacing
\setlength{\parskip}{6pt}
\setlength{\parindent}{0pt}

% Macros
\newcommand{\benchmark}{Chatio LLM Benchmark}
\newcommand{\product}{Chatio}
\newcommand{\version}{v0.9 (Draft)}

%==============================================================================%
% Document
%==============================================================================%
\begin{document}

\begin{titlepage}
  \centering
  {\Large \textbf{\benchmark}\\[6pt]
  System Card}\\[1.0cm]
  {\normalsize \version \quad|\quad \today}\\[1.0cm]
  {\small Maintained by the Chatio Benchmark Team}\\[1.5cm]

  \begin{infobox}{At a Glance}
    \begin{itemize}
      \item Goal: Evaluate real-world assistant usability and helpfulness across everyday tasks, not just academic proxies \cite{prompteng2024,livebench2025,e-bench2025}.
      \item Coverage: Five categories spanning core assistant skills:
      Emotional Support; Formatting Assistance; Creative Writing; Reading Comprehension; General Help and Knowledge.
      \item Evaluation: Mixed-method scoring with human review and automated methods, including LLM-as-a-Judge where appropriate \cite{galileo-judge-2024,geval-2023,citadel-2024}.
      \item Reporting: Category-level and overall normalized scores with balanced weighting.
      \item Scope limits: English-only, single-turn prompts; text-based tasks (no code, math Olympiad, or multimodal in this version).
      \item Update cadence: Periodic content refreshes to reduce test-set contamination and overfitting \cite{prompteng2024,livebench2025}.
      \item Intended use: Model comparison, regression tracking, and capability mapping; not a certification for high-stakes deployment.
    \end{itemize}
  \end{infobox}

  \vfill
  {\small This system card documents the intent, design, evaluation methods, ethics, limitations, and update policy for the \product{} benchmark.}
\end{titlepage}

\tableofcontents
\newpage

\section{Purpose and Scope}

The \benchmark{} evaluates language models on practical, real-world tasks that reflect everyday user needs. Its focus is real-world usability: measuring how well models handle messy, open-ended user queries and typical assistant workflows, as opposed to purely laboratory-style tests. This addresses a known gap where models that score well on academic benchmarks may falter in everyday use \cite{prompteng2024,livebench2025,e-bench2025}.

\product{} emphasizes a broad range of common assistant capabilities (from empathetic support to factual Q\&A) so that high benchmark scores translate into genuinely helpful behavior in real use. By prioritizing realistic tasks and user-centric criteria, the benchmark aims to promote AI systems that are not only technically proficient but also truly useful in day-to-day interactions.

\section{Task Design and Coverage}

\product{} is organized into five categories representing core assistant utility \cite{livebench2025,e-bench2025}:

\subsection{Emotional Support}
Models provide empathetic, supportive responses to users in distress or seeking comfort. Emphasis is on emotional intelligence (e.g., validation, encouragement, active listening) and maintaining a respectful, calming tone. Success indicates the model can handle sensitive conversations with care.

\subsection{Formatting Assistance}
Tasks require transforming or producing text in a specified format or style (e.g., lists, tables, Markdown/JSON). This assesses instruction-following, attention to detail, and compliance with formatting constraints helpful for email drafting, note-taking, and structured data exchange.

\subsection{Creative Writing}
Models generate original creative content (short stories, poems, dialogues, or imaginative scenarios). Evaluation emphasizes creativity, coherence, expression, and originality while maintaining logical flow—reflecting real use for brainstorming or entertainment.

\subsection{Reading Comprehension}
Given a passage or article, models answer questions, summarize, or explain. This measures the ability to understand and use provided context, accurately extract information, follow narratives, and avoid hallucinations—useful for digesting news, documents, or learning materials.

\subsection{General Help and Knowledge}
Everyday informational or instructional queries across an open domain (e.g., “Fixing a leaky faucet,” “Explain X in simple terms”). This tests breadth of knowledge, reasoning, clarity, and actionable helpfulness.

These categories collectively span social-emotional skills, instruction-following and formatting, creative generation, comprehension, and general reasoning, providing a well-rounded assessment and encouraging balanced model capabilities.

\section{Evaluation Methodology}

\subsection{Overview}
\product{} uses multi-faceted evaluation combining human judgment with automated metrics to balance depth (qualitative nuance) and breadth (scalable coverage). Each task instance is evaluated on criteria suited to the category.

\subsection{Human Review}
For subjective or nuanced tasks (e.g., emotional support, creative writing), trained annotators score outputs using clear rubrics. Criteria include empathy and appropriateness (support), or coherence and creativity (writing). Human judgment remains the reference standard for tone, context, and usefulness \cite{citadel-2024}. Multiple reviewers may score a sample to increase reliability. Due to time and cost, human review is applied where it adds the most value and captures subtleties automated methods may miss \cite{prompteng2024}.

\subsection{Automated Evaluation}
For structured or objective tasks, programmatic checks and LLM-based evaluators provide efficiency and consistency:
\begin{itemize}
  \item Rule- or reference-based checks (e.g., exact formatting compliance, string overlap for comprehension questions).
  \item LLM-as-a-Judge: A strong model (e.g., GPT-class) is prompted to rate another model’s output on predefined criteria such as correctness, relevance, and style \cite{galileo-judge-2024,geval-2023}. Studies report useful alignment with human judgments for nuanced criteria when carefully prompted \cite{geval-2023}.
\end{itemize}
Automated methods enable rapid, consistent scoring but can misjudge context or subtleties; their results are interpreted with caution and triangulated with human ratings where appropriate \cite{prompteng2024,citadel-2024}.

\subsection{Score Aggregation and Reporting}
For each prompt, multiple ratings (if present) are averaged to yield an instance score. Scores are then aggregated across tasks and categories. Categories are balanced (e.g., equal-weighted) so excelling in one area does not mask weaknesses elsewhere. Final results report:
\begin{itemize}
  \item An overall normalized score (e.g., 0–100).
  \item Category-level scores to reveal strengths and weaknesses.
  \item Notes on evaluation mix (human/automated) for transparency.
\end{itemize}
This blended approach reflects emerging best practices: use automated methods for scale and consistency, validate with human review, and combine signals thoughtfully \cite{citadel-2024,prompteng2024}.

\section{Ethical Considerations}

\subsection{Bias and Fairness}
LLMs can inadvertently produce biased or stereotypical content. \product{} includes tasks that surface differential treatment across genders, backgrounds, or occupations, and penalizes harmful stereotypes or discriminatory language. Human evaluators apply fairness guidelines and watch for subtle biases. Bias issues have been prominent in real deployments \cite{phare-2025}. The benchmark incentivizes equitable, respectful language across identities.

\subsection{Cultural Sensitivity}
Global assistants must handle diverse cultures without a narrow viewpoint. Many LLMs are trained primarily on English/Western data and can underperform on less-represented contexts \cite{blend-2024}. \product{} includes prompts referencing non-Western holidays, local customs, and everyday life across regions, rewarding culturally appropriate, contextually aware responses. Cultural insensitivity and stereotypes are considered failure modes \cite{blend-2024,phare-2025}.

\subsection{Safety and Harmful Content}
The benchmark avoids tasks encouraging disallowed content. Evaluators consider safety risks; blatantly harmful or abusive responses are noted and penalized. Helpfulness must align with ethics: superior performance requires both capability and responsible behavior.

\section{Limitations and Known Gaps}

No benchmark covers everything; key limitations include:

\begin{itemize}
  \item \textbf{Scope of Tasks:} Focused on conversational, text-only assistant tasks. No explicit evaluation of software engineering, advanced math problem-solving, or multimodal understanding in this version.
  \item \textbf{Language Coverage:} Prompts and evaluations are primarily English. Performance in other languages is not measured; multilingual variants are a planned extension.
  \item \textbf{Dialogue Depth:} Most prompts are single-turn. Long-horizon dialogue coherence, memory, and adaptation to follow-ups are not explicitly tested.
  \item \textbf{Evaluation Imperfections:} Human review can be subjective; automated judges can have blind spots. Clear rubrics and mixed methods reduce but do not eliminate variance \cite{galileo-judge-2024,geval-2023,citadel-2024}.
  \item \textbf{Overfitting and Familiarity:} Public benchmarks risk test-set contamination and overfitting. \product{} mitigates with a dynamic update policy but cannot fully preclude memorization \cite{prompteng2024,livebench2025}.
\end{itemize}

Users should treat \product{} as one tool among many, complemented by domain-specific, multilingual, and multi-turn evaluations as needed.

\section{Update and Versioning Policy}

To remain robust and relevant, \product{} evolves over time. Task sets are periodically refreshed—adding, modifying, or retiring items—to reduce overfitting and test familiarity. This dynamic approach counters contamination as content becomes known or incorporated into training data \cite{prompteng2024,livebench2025}.

Practically:
\begin{itemize}
  \item New tasks are introduced on a regular cadence (e.g., every few months), inspired by emerging user needs, failure modes, and under-represented scenarios.
  \item Core domains (the five categories) remain stable; specific prompts are refreshed incrementally to preserve comparability while maintaining challenge.
  \item Versions are documented; evaluations should cite the benchmark version used.
  \item Updates avoid drastic domain shifts that would unfairly favor or penalize particular models.
\end{itemize}

Participants are encouraged to evaluate on the latest version and avoid hand-tuning models to prior public task sets. The goal is to measure genuine generalization and real-world utility over time \cite{livebench2025}.

\section{Responsible Use and Disclosure}

\begin{itemize}
  \item \textbf{Intended Use:} Comparative evaluation of assistant-like LLMs, regression tracking, and capability mapping.
  \item \textbf{Not Intended For:} High-stakes deployment certification, safety approval, or domain-specific compliance.
  \item \textbf{Transparency:} Report model version, decoding parameters, and benchmark version. Include notes on human vs.\ automated evaluation proportions.
  \item \textbf{Reproducibility:} Use provided evaluation scripts and templates when available. Random seeds and sampling settings should be documented.
\end{itemize}

\section*{Acknowledgments}
We thank the reviewers and contributors who helped refine the task set, rubrics, and scoring pipelines.

\section*{How to Cite}
Chatio Benchmark Team. “Chatio LLM Benchmark: System Card.” \version, \today.

\newpage
\begin{thebibliography}{99}

\bibitem{prompteng2024}
PromptEngineering.org. “Challenges and Innovations in Language Model Benchmarking and Generalization.” 2024.

\bibitem{livebench2025}
Emergent Mind. “LiveBench: Dynamic LLM Benchmark Suite.” 2025.

\bibitem{galileo-judge-2024}
Galileo AI Blog. “LLM-as-a-Judge vs Human Evaluation.” 2024.

\bibitem{geval-2023}
Confident AI Blog. “G-Eval Simply Explained: LLM-as-a-Judge for LLM Evaluation.” 2023.

\bibitem{citadel-2024}
Citadel AI Blog. “Announcing Lens for LLMs: Combining Human and Automated LLM Evaluation.” 2024.

\bibitem{phare-2025}
Giskard (Phare Benchmark). “LLMs recognise bias but also reproduce harmful stereotypes.” 2025.

\bibitem{blend-2024}
Myung et al. “BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages.” arXiv, 2024.

\bibitem{e-bench2025}
Galileo AI Blog. “Evaluating LLM Ease-of-Use Through the E-Bench Framework.” 2025.

\end{thebibliography}

\end{document}
